## Week 2

### Highlights

- Tricks for improving gradient descend

1. Feature scaling and mean normalization

2. Scrutinize the learning rate graph (Cost function and number of iterations):

  - If J(theta) is decreasing, normal
  - If J(theta) is increasing, use smaller alpha
  - If J(theta) is increasing and decreasing alternatively, use smaller alpha
